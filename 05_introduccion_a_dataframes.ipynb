{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![img/pythonista.png](img/pythonista.png)](https://www.pythonista.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a *Dataframes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Intro a Dataframes\").getOrCreate()\n",
    "ct = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *dataframes* son un concepto compartido entre plataformas como *R*, *Pandas* y *Scala*. Son estructuras tabulares en las que todos los datos de una columna comparten el mismo tipo de datos. Cada columna tiene un título y cada renglón tiene un índice. A la descripción de los tipos de datos de cada columna de un *dataframe* se le conoce como esquema (*schema*).\n",
    "\n",
    "*PySPark* tiene la capacidad de poder manejar *dataframes* tanto de *Pandas* como de *Spark* e incluso cuenta con una [*API*](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) que optimiza la interacción entre ambos tipos de *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los *dataframes* de *Spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *dataframe* de *Spark* es un objeto instanciado de la clase [```pyspark.sql.DataFrame```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de *dataframes*.\n",
    "\n",
    "La función [```spark.createDataFrame()```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html) permite crear dataframes a partir de objetos que se ingresan como argumentos.\n",
    "\n",
    "\n",
    "```pyspark\n",
    "df = spark.createDataFrame(data=<obj>, <títulos columnas>, schema=<esquema>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<obj>``` es un objeto que represente una estructura tabular el cual puede ser:\n",
    "    * Una colección de *Python*.\n",
    "    * Un *dataframe* de *Pandas*.\n",
    "    * Un *RDD* de *Spark*.\n",
    "    \n",
    "Cabe hacer notar que los *dataframes* de *Spark* se construyen de forma perezosa, por lo que aún cuando sean definidos, estos no serán creados hasta que sean requeridos.\n",
    "    \n",
    "Por convención se utiliza el nombre ```df``` para designar undataframe genérico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los objetos ```Row ``` y ```Column```.\n",
    "\n",
    "Los *dataframes* de *Spark* están compuestos por 2 tipos de objetos.\n",
    "\n",
    "* Los objetos de tipo ```pyspark.sql.Row``` que representan a cada renglón del *dataframe*.\n",
    "* Los objetos de tipo ```pyspark.sql.Column``` que representan a cada columna del *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.show()```.\n",
    "\n",
    "El método ```df.show()``` muestra los primeros ```n``` números de un *dataframe* de *Spark*.\n",
    "\n",
    "```\n",
    "df.show(<n>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<n>``` es el número de renglones desplegados. El valor por defecto es ```20```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas crearán un *dataframe* de *Spark* a partir de un *dataframe* de *Pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se creará el *dataframe* de *Pandas* llamado ```pandas_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame({'Dirección':('Sur', 'Norte', 'Sur', 'Este'),\n",
    "              'Rumbo':('Este', 'Noroeste', 'Norte', 'Norte'),\n",
    "             'Pasajeros':(12, 24, 32, 5),\n",
    "             'Documentado':(True, None, True, False) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se creará el *dataframe* de *Spark* a partir de ```pandas_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas creará un *dataframe* de *Spark* a partir dde un *RDD* que contiene una colección de objetos ```Row```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = ct.parallelize((Row('Sur', 'Este', '12', True),\n",
    "                     Row('Norte', 'Noroeste', '24', None),\n",
    "                     Row('Sur', 'Norte', '32', True),\n",
    "                     Row('Este', 'Norte', '5', False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *dataset* a partir del *RDD* y se infgresará como argumento el nombre de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, \n",
    "                ['Dirección', 'Rumbo', 'Pasajeros', 'Documentado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de una columna de un *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible acceder a una columna de un *dataframe* usando su nombre o usando su número de índice consecutivo iniciando desde ```0```. \n",
    "\n",
    "```\n",
    "df.<Nombre Columna>\n",
    "```\n",
    "\n",
    "```\n",
    "df[<n>]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Rumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El atributo ```df.schema```.\n",
    "\n",
    "El atributo ```df.schema``` contiene la estructura del *schema* del *dataframe* como una instancia de la clase ```pyspark.sql.types.StructType``` que contiene una colección de instancias de tipo ```pyspark.sql.types.StructField```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.printSchema()```.\n",
    "\n",
    "El método ```df.printSchema()``` despliega una cadena de caractéres describiendo el *schema* del *dataframe*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipado de *Spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Spark* cuenta con distintos tipos de datos que extienden a los tipo nativos de *Scala*, *Python* y *R*. El módulo ```pyspark.sql.types``` contiene a todas las clases correspondientes a dichos tipo.\n",
    "\n",
    "El siguente enlace apunta a la referncia de tipos de datos de *Spark*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda importará todos los tipos de datos de *PySpark* al entorno de esta *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de *schemas* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El tipo ```pyspark.api.types.StructField```.\n",
    "\n",
    "El tipo ```pyspark.api.types.StructField``` permite definir esquemas tanto para describir las columnas de un *dataframe* como para describir *schemas* complejos como los que se pueden encontrar en *YAML* o *JSON*.    \n",
    "Las estructuras complejas que pueden crearse en los *schemas* son contenidas dentro de un objeto de tipo ```pyspark.api.types.StructType```.\n",
    "\n",
    "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.StructField.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tipo```StructType```.\n",
    "\n",
    "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.StructType.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda describe un *schema* para el *dataframe* ```df```en le que la columnas ```Pasajeros``` es un entero que va de ```-128```a ```127```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Dirección', StringType(), True), \n",
    "                     StructField('Rumbo', StringType(), True), \n",
    "                     StructField('Pasajeros', ByteType(), True), \n",
    "                     StructField('Documentado', BooleanType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará una *dataframe* con el *schema* correspondiente a ```schema```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos para *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de *PySpark*  es psoible leer y crear dataframes a partir de distintos formatos de archivo que describan estructuras tabulares. El atributo  [```spark.read```](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html) contiene una familia de métodos que pueden importar y convertir en *dataframes* diversos tipos de documento.\n",
    "\n",
    "```\n",
    "df = spark.read.<método>(<ruta>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuentes de datos compatibles para los *dataframes*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda importará los datos del archivo ```data/data_covid.parquet```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/data_covid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación se usará el método ```df.toPandas()``` para mostrar el *dataframe* ```df``` de mejor manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('index', DateType(), True), StructField('AGUASCALIENTES', LongType(), True), StructField('BAJA CALIFORNIA', LongType(), True), StructField('BAJA CALIFORNIA SUR', LongType(), True), StructField('CAMPECHE', LongType(), True), StructField('CHIAPAS', LongType(), True), StructField('CHIHUAHUA', LongType(), True), StructField('DISTRITO FEDERAL', LongType(), True), StructField('COAHUILA', LongType(), True), StructField('COLIMA', LongType(), True), StructField('DURANGO', LongType(), True), StructField('GUANAJUATO', LongType(), True), StructField('GUERRERO', LongType(), True), StructField('HIDALGO', LongType(), True), StructField('JALISCO', LongType(), True), StructField('MEXICO', LongType(), True), StructField('MICHOACAN', LongType(), True), StructField('MORELOS', LongType(), True), StructField('NAYARIT', LongType(), True), StructField('NUEVO LEON', LongType(), True), StructField('OAXACA', LongType(), True), StructField('PUEBLA', LongType(), True), StructField('QUERETARO', LongType(), True), StructField('QUINTANA ROO', LongType(), True), StructField('SAN LUIS POTOSI', LongType(), True), StructField('SINALOA', LongType(), True), StructField('SONORA', LongType(), True), StructField('TABASCO', LongType(), True), StructField('TAMAULIPAS', LongType(), True), StructField('TLAXCALA', LongType(), True), StructField('VERACRUZ', LongType(), True), StructField('YUCATAN', LongType(), True), StructField('ZACATECAS', LongType(), True), StructField('Nacional', LongType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").schema(schema).csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de funciones de SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').where(df.AGUASCALIENTES >1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').where(df.index == date(2020,12,20)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2022.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
